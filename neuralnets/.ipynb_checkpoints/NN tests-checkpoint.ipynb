{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff98043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from models import *\n",
    "from nn_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "684b08aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if os.path.exists('data/MNIST'):\n",
    "    download = False\n",
    "else:\n",
    "    download = True\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=download,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "testing_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=download,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "103293fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "\n",
    "bs = 50\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=bs, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9749934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose model\n",
    "\n",
    "net = Model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "784acb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose loss function, optimizer, and scheduler\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(net.parameters(), lr=0.00001, momentum=0.9)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d9bbfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 1 (5.8s)   loss 1.1606492530554533\n",
      "End of epoch 2 (5.4s)   loss 0.4882569048802058\n",
      "End of epoch 3 (5.7s)   loss 0.3815339595700304\n",
      "End of epoch 4 (5.3s)   loss 0.3373848415352404\n",
      "End of epoch 5 (5.4s)   loss 0.309823696265618\n",
      "End of epoch 6 (5.4s)   loss 0.2888928384023408\n",
      "End of epoch 7 (5.4s)   loss 0.27229497623319426\n",
      "End of epoch 8 (5.4s)   loss 0.2572574558760971\n",
      "End of epoch 9 (5.3s)   loss 0.24487201978762946\n",
      "End of epoch 10 (5.3s)   loss 0.23377507933415473\n",
      "End of epoch 11 (5.3s)   loss 0.2238849698069195\n",
      "End of epoch 12 (5.4s)   loss 0.21484606668973963\n",
      "End of epoch 13 (5.3s)   loss 0.20678718929644674\n",
      "End of epoch 14 (5.3s)   loss 0.1991863609602054\n",
      "End of epoch 15 (5.4s)   loss 0.1921044339782869\n",
      "End of epoch 16 (4.9s)   loss 0.185840797345154\n",
      "End of epoch 17 (4.9s)   loss 0.17955802922292302\n",
      "End of epoch 18 (5.6s)   loss 0.1739278336831679\n",
      "End of epoch 19 (5.6s)   loss 0.16847998140069345\n",
      "End of epoch 20 (5.9s)   loss 0.16327745226056625\n",
      "End of epoch 21 (5.4s)   loss 0.15845856925550228\n",
      "End of epoch 22 (5.4s)   loss 0.15393752249733855\n",
      "End of epoch 23 (5.4s)   loss 0.1496905137016438\n",
      "End of epoch 24 (5.4s)   loss 0.14550455661956221\n",
      "End of epoch 25 (5.3s)   loss 0.1416236693932054\n",
      "End of epoch 26 (5.4s)   loss 0.13784904905129225\n",
      "End of epoch 27 (5.4s)   loss 0.1343331726298978\n",
      "End of epoch 28 (5.4s)   loss 0.13100733056819688\n",
      "End of epoch 29 (5.4s)   loss 0.1277775587622697\n",
      "End of epoch 30 (5.5s)   loss 0.12474464997571583\n",
      "End of epoch 31 (5.3s)   loss 0.12160463119313741\n",
      "End of epoch 32 (5.6s)   loss 0.11899751050552974\n",
      "End of epoch 33 (5.4s)   loss 0.11606560428626836\n",
      "End of epoch 34 (5.6s)   loss 0.11350908948884655\n",
      "End of epoch 35 (5.9s)   loss 0.11132828427789111\n",
      "End of epoch 36 (5.6s)   loss 0.1087894541840069\n",
      "End of epoch 37 (5.6s)   loss 0.1064819377140763\n",
      "End of epoch 38 (5.6s)   loss 0.10432374010095373\n",
      "End of epoch 39 (5.6s)   loss 0.10215802112749467\n",
      "End of epoch 40 (5.4s)   loss 0.09999690782395192\n",
      "End of epoch 41 (5.7s)   loss 0.09809780204086564\n",
      "End of epoch 42 (5.5s)   loss 0.09622729593034213\n",
      "End of epoch 43 (5.7s)   loss 0.09431424103716078\n",
      "End of epoch 44 (5.4s)   loss 0.09250266119643735\n",
      "End of epoch 45 (5.3s)   loss 0.09063764481378409\n",
      "End of epoch 46 (5.4s)   loss 0.08912054628599435\n",
      "End of epoch 47 (5.5s)   loss 0.08735636104208727\n",
      "End of epoch 48 (5.4s)   loss 0.08578274672307695\n",
      "End of epoch 49 (5.4s)   loss 0.08426933561529343\n",
      "End of epoch 50 (5.6s)   loss 0.08279970881179906\n",
      "End of epoch 51 (5.4s)   loss 0.07962993819499388\n",
      "End of epoch 52 (5.5s)   loss 0.07917387450036283\n",
      "End of epoch 53 (5.3s)   loss 0.07889860029875612\n",
      "End of epoch 54 (5.5s)   loss 0.0785709289407047\n",
      "End of epoch 55 (5.3s)   loss 0.07829296268289909\n",
      "End of epoch 56 (5.5s)   loss 0.07798667130875402\n",
      "End of epoch 57 (5.4s)   loss 0.07768218606555213\n",
      "End of epoch 58 (5.5s)   loss 0.07742392084522483\n",
      "End of epoch 59 (5.3s)   loss 0.07710837817750871\n",
      "End of epoch 60 (5.3s)   loss 0.07678778054541908\n",
      "End of epoch 61 (5.4s)   loss 0.07660425858882566\n",
      "End of epoch 62 (5.3s)   loss 0.07632602876091066\n",
      "End of epoch 63 (5.5s)   loss 0.07597837947270213\n",
      "End of epoch 64 (5.4s)   loss 0.07570573215857924\n",
      "End of epoch 65 (5.3s)   loss 0.07547151409477616\n",
      "End of epoch 66 (5.4s)   loss 0.07521612963988446\n",
      "End of epoch 67 (5.6s)   loss 0.07487501642395121\n",
      "End of epoch 68 (5.3s)   loss 0.07468869206010519\n",
      "End of epoch 69 (5.4s)   loss 0.07443120096519124\n",
      "End of epoch 70 (5.6s)   loss 0.07411947893347436\n",
      "End of epoch 71 (5.6s)   loss 0.07390320291975513\n",
      "End of epoch 72 (5.4s)   loss 0.07359116135436732\n",
      "End of epoch 73 (5.3s)   loss 0.07338916929807358\n",
      "End of epoch 74 (5.8s)   loss 0.07308889377124918\n",
      "End of epoch 75 (5.4s)   loss 0.07282169402965034\n",
      "End of epoch 76 (5.3s)   loss 0.07260599392660273\n",
      "End of epoch 77 (5.6s)   loss 0.07238264056562911\n",
      "End of epoch 78 (5.3s)   loss 0.07208099842187948\n",
      "End of epoch 79 (5.5s)   loss 0.07183585541284022\n",
      "End of epoch 80 (5.4s)   loss 0.07152417862865453\n",
      "End of epoch 81 (5.7s)   loss 0.07130176160523358\n",
      "End of epoch 82 (5.5s)   loss 0.0711546206047448\n",
      "End of epoch 83 (5.4s)   loss 0.07085706641858754\n",
      "End of epoch 84 (5.3s)   loss 0.07060099694140566\n",
      "End of epoch 85 (5.3s)   loss 0.07039733556409677\n",
      "End of epoch 86 (5.4s)   loss 0.0701369611635649\n",
      "End of epoch 87 (5.8s)   loss 0.06994748099377224\n",
      "End of epoch 88 (5.5s)   loss 0.06964000038977247\n",
      "End of epoch 89 (5.5s)   loss 0.0694092419631003\n",
      "End of epoch 90 (5.3s)   loss 0.06917475499173936\n",
      "End of epoch 91 (5.6s)   loss 0.06894798783236183\n",
      "End of epoch 92 (5.3s)   loss 0.06869235588160033\n",
      "End of epoch 93 (5.3s)   loss 0.06849854873454508\n",
      "End of epoch 94 (5.3s)   loss 0.06827330795543579\n",
      "End of epoch 95 (5.3s)   loss 0.06802656947014232\n",
      "End of epoch 96 (5.7s)   loss 0.06781731897848658\n",
      "End of epoch 97 (5.6s)   loss 0.06752329168297971\n",
      "End of epoch 98 (5.7s)   loss 0.0673290912102675\n",
      "End of epoch 99 (5.6s)   loss 0.06710850273045556\n",
      "End of epoch 100 (5.5s)   loss 0.06695005754746186\n",
      "End of epoch 101 (5.5s)   loss 0.06625572557910346\n",
      "End of epoch 102 (5.4s)   loss 0.06616840005561243\n",
      "End of epoch 103 (5.4s)   loss 0.0661335498734843\n",
      "End of epoch 104 (5.3s)   loss 0.06607759093632921\n",
      "End of epoch 105 (5.4s)   loss 0.06602696656967358\n",
      "End of epoch 106 (6.2s)   loss 0.06599788333541558\n",
      "End of epoch 107 (5.8s)   loss 0.06593453587032855\n",
      "End of epoch 108 (5.5s)   loss 0.06590110850287602\n",
      "End of epoch 109 (5.3s)   loss 0.06584985231476215\n",
      "End of epoch 110 (5.3s)   loss 0.06581830590575312\n",
      "End of epoch 111 (5.3s)   loss 0.06576669312101634\n",
      "End of epoch 112 (5.5s)   loss 0.06571774399063239\n",
      "End of epoch 113 (5.4s)   loss 0.06567913318246914\n",
      "End of epoch 114 (5.3s)   loss 0.06560938345462394\n",
      "End of epoch 115 (5.4s)   loss 0.06558566298258181\n",
      "End of epoch 116 (5.7s)   loss 0.06554278892794779\n",
      "End of epoch 117 (5.5s)   loss 0.06550130421683813\n",
      "End of epoch 118 (5.5s)   loss 0.0654461638899132\n",
      "End of epoch 119 (5.5s)   loss 0.06541898212589634\n",
      "End of epoch 120 (5.5s)   loss 0.06535253551225954\n",
      "End of epoch 121 (5.3s)   loss 0.06532301801586679\n",
      "End of epoch 122 (5.5s)   loss 0.06528644615085795\n",
      "End of epoch 123 (5.4s)   loss 0.0652418275421951\n",
      "End of epoch 124 (5.6s)   loss 0.06517913728331526\n",
      "End of epoch 125 (5.5s)   loss 0.06515941926142356\n",
      "End of epoch 126 (5.5s)   loss 0.06510736198727196\n",
      "End of epoch 127 (5.5s)   loss 0.06506958112857926\n",
      "End of epoch 128 (5.4s)   loss 0.06501648336842966\n",
      "End of epoch 129 (5.2s)   loss 0.06497092235037902\n",
      "End of epoch 130 (5.3s)   loss 0.06493884720218679\n",
      "End of epoch 131 (5.4s)   loss 0.06489228999222783\n",
      "End of epoch 132 (5.3s)   loss 0.06483833142944301\n",
      "End of epoch 133 (5.4s)   loss 0.06481628297207256\n",
      "End of epoch 134 (5.3s)   loss 0.06476353571362173\n",
      "End of epoch 135 (5.1s)   loss 0.06471380979909251\n",
      "End of epoch 136 (5.2s)   loss 0.0646856810406704\n",
      "End of epoch 137 (5.4s)   loss 0.06463898210514647\n",
      "End of epoch 138 (5.3s)   loss 0.06459182237177932\n",
      "End of epoch 139 (5.4s)   loss 0.06455343397858088\n",
      "End of epoch 140 (5.3s)   loss 0.06451993480072513\n",
      "End of epoch 141 (5.3s)   loss 0.06446827125017686\n",
      "End of epoch 142 (5.4s)   loss 0.06441876321545957\n",
      "End of epoch 143 (5.3s)   loss 0.06437821193637015\n",
      "End of epoch 144 (5.3s)   loss 0.06433136434915165\n",
      "End of epoch 145 (5.4s)   loss 0.06429415284190326\n",
      "End of epoch 146 (5.3s)   loss 0.06425281069319075\n",
      "End of epoch 147 (5.2s)   loss 0.06419877096544951\n",
      "End of epoch 148 (5.4s)   loss 0.06416540602668344\n",
      "End of epoch 149 (5.6s)   loss 0.06413729697582311\n",
      "End of epoch 150 (6.0s)   loss 0.06407406572912198\n",
      "End of epoch 151 (5.5s)   loss 0.06393800284267248\n",
      "End of epoch 152 (5.3s)   loss 0.0639211105221572\n",
      "End of epoch 153 (5.4s)   loss 0.06391029470076319\n",
      "End of epoch 154 (5.4s)   loss 0.06390204816377566\n",
      "End of epoch 155 (5.4s)   loss 0.06389428962953389\n",
      "End of epoch 156 (5.4s)   loss 0.0638843266213856\n",
      "End of epoch 157 (5.4s)   loss 0.06387622940373451\n",
      "End of epoch 158 (5.4s)   loss 0.06386777633568272\n",
      "End of epoch 159 (5.3s)   loss 0.06385915850502594\n",
      "End of epoch 160 (5.3s)   loss 0.06385086825330898\n",
      "End of epoch 161 (5.3s)   loss 0.06384086118991641\n",
      "End of epoch 162 (5.5s)   loss 0.06383309744453679\n",
      "End of epoch 163 (5.3s)   loss 0.06382617990272896\n",
      "End of epoch 164 (5.3s)   loss 0.06381708852267669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of epoch 165 (5.3s)   loss 0.06380862641633334\n",
      "End of epoch 166 (5.4s)   loss 0.06379887823131866\n",
      "End of epoch 167 (5.4s)   loss 0.06379164387355558\n",
      "End of epoch 168 (5.4s)   loss 0.06378284215461463\n",
      "End of epoch 169 (5.3s)   loss 0.06377601457332882\n",
      "End of epoch 170 (5.2s)   loss 0.06376598378643393\n",
      "End of epoch 171 (5.7s)   loss 0.06375798837048934\n",
      "End of epoch 172 (6.3s)   loss 0.06374953042036699\n",
      "End of epoch 173 (6.2s)   loss 0.06373915431826996\n",
      "End of epoch 174 (5.4s)   loss 0.06373243822910202\n",
      "End of epoch 175 (5.4s)   loss 0.06372348444187083\n",
      "End of epoch 176 (5.4s)   loss 0.06371568613414032\n",
      "End of epoch 177 (5.4s)   loss 0.06370431731047574\n",
      "End of epoch 178 (5.3s)   loss 0.0636986191557177\n",
      "End of epoch 179 (5.4s)   loss 0.06369162737188162\n",
      "End of epoch 180 (5.4s)   loss 0.06368209009291605\n",
      "End of epoch 181 (5.4s)   loss 0.06367269167560152\n",
      "End of epoch 182 (5.3s)   loss 0.0636636374508574\n",
      "End of epoch 183 (5.3s)   loss 0.06365587771909001\n",
      "End of epoch 184 (5.3s)   loss 0.06364702185518885\n",
      "End of epoch 185 (5.3s)   loss 0.06363889774424024\n",
      "End of epoch 186 (5.4s)   loss 0.06363172049352822\n",
      "End of epoch 187 (5.4s)   loss 0.0636235380822715\n",
      "End of epoch 188 (5.3s)   loss 0.06361464644685233\n",
      "End of epoch 189 (5.4s)   loss 0.06360591636039317\n",
      "End of epoch 190 (5.4s)   loss 0.06359593303583097\n",
      "End of epoch 191 (5.4s)   loss 0.06358806614608814\n",
      "End of epoch 192 (5.4s)   loss 0.06358121388145567\n",
      "End of epoch 193 (5.3s)   loss 0.06356899677581775\n",
      "End of epoch 194 (5.4s)   loss 0.06356549894107351\n",
      "End of epoch 195 (5.4s)   loss 0.06355423487984808\n",
      "End of epoch 196 (5.4s)   loss 0.06354629649722483\n",
      "End of epoch 197 (5.3s)   loss 0.06353759382463371\n",
      "End of epoch 198 (5.3s)   loss 0.0635304936634687\n",
      "End of epoch 199 (5.3s)   loss 0.06352126915279466\n",
      "End of epoch 200 (5.3s)   loss 0.0635123017480752\n",
      "\n",
      "Finished Training\n",
      "Time taken: 18.11 minutes\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "net = train(net, train_dataloader, 200, criterion, optimizer, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef66fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9687\n",
      "\n",
      "Previous best accuracy: 0.9792\n"
     ]
    }
   ],
   "source": [
    "# make predictions and save best model\n",
    "\n",
    "acc = accuracy(net, test_dataloader)\n",
    "print(f'Accuracy: {round(acc, 5)}\\n')\n",
    "\n",
    "with open('models/best_accuracy.npy', 'rb') as f:\n",
    "    best_acc = float(np.load(f))\n",
    "    \n",
    "print(f'Previous best accuracy: {round(best_acc, 5)}')\n",
    "    \n",
    "if acc > best_acc:\n",
    "    torch.save(net, 'models/best_model.pth')\n",
    "    with open('models/best_accuracy.npy', 'wb') as f:\n",
    "        np.save(f, acc)\n",
    "        \n",
    "    print('\\nNew model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abe5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f04d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0083e188",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95401f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "gpu = torch.cuda.is_available()\n",
    "print(gpu)\n",
    "if gpu:\n",
    "    print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
